{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preface: I used Python 3 (v 5.5.0) to write this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating and Compressing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I used ```numpy``` to generate random data and ```gzip```/```bzip2```/```pbzip2```/```ArithmeticCompress``` to compress this data. \n",
    "\n",
    "A general outline of this lab:\n",
    "1. Simulating random binary, nucleotide, and protein data\n",
    "2. Compress random data\n",
    "3. Questions about compression\n",
    "4. Compressing HIV gp120 homolog data\n",
    "5. Estimations for larger datasets (1000 TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating Random Data\n",
    "\n",
    "I first generated 100 MB of random binary data for each set 100%, 90%, 80%, 70%, 60%, and 50% zeros. Output in bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 100% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Hundred = np.random.choice([0, 1], size=[838860800], replace=True, p=[1.0, 0.0])\n",
    "Hundred = np.packbits(Hundred)\n",
    "print(Hundred)\n",
    "open(\"100_Zeros\", \"wb\").write(Hundred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64  32   0 ...   1   4 136]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 90% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Ninety = np.random.choice([0, 1], size=[838860800], replace=True, p=[0.9, 0.1])\n",
    "Ninety = np.packbits(Ninety)\n",
    "print(Ninety)\n",
    "open(\"90_Zeros\", \"wb\").write(Ninety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42 170 128 ...  65  33  65]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 80% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Eighty = np.random.choice([0, 1], size=[838860800], replace=True, p=[0.8, 0.2])\n",
    "Eighty = np.packbits(Eighty)\n",
    "print(Eighty)\n",
    "open(\"80_Zeros\", \"wb\").write(Eighty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 96  2 ... 16 51 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 70% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Seventy = np.random.choice([0, 1], size=[838860800], replace=True, p=[0.7, 0.3])\n",
    "Seventy = np.packbits(Seventy)\n",
    "print(Seventy)\n",
    "open(\"70_Zeros\", \"wb\").write(Seventy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32   7 186 ...  49  96 168]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 60% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Sixty = np.random.choice([0, 1], size=[838860800], replace=True, p=[0.6, 0.4])\n",
    "Sixty = np.packbits(Sixty)\n",
    "print(Sixty)\n",
    "open(\"60_Zeros\", \"wb\").write(Sixty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188 161  95 ...  73  20 241]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 MB of random data containing 50% zeros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Fifty = np.random.choice([0, 1], size=[838860800], replace=True, p=[0.5, 0.5])\n",
    "Fifty = np.packbits(Fifty)\n",
    "print(Fifty)\n",
    "open(\"50_Zeros\", \"wb\").write(Fifty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I generated random DNA and protein data (100 million letters long) and saved them as FASTA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'T' 'G' ... 'C' 'T' 'A']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 million random nucleotide sequence\n",
    "\n",
    "DNA = np.random.choice(['A', 'T', 'G', 'C'], size=100000000, replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "print(DNA)\n",
    "open(\"nt_seq.fa\", \"w\").write(\"\".join(DNA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q' 'A' 'I' ... 'V' 'Q' 'C']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 million random amino acid sequence\n",
    "\n",
    "Pro = np.random.choice(['G','A','V','L','I','D','E','N','Q','P','F','W','K','C','M','Y','R','H','S','T'], \n",
    "                       size = 100000000, replace=True, p=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "                                                          0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "                                                          0.05, 0.05, ])\n",
    "print(Pro)\n",
    "open(\"pro_seq.fa\", \"w\").write(\"\".join(Pro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compressing Random Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used **gzip**, **bzip2**, **pbzip2**, and **ArithmeticCompress** on **Terminal** to compress the random data sets generated in Part 1.\n",
    "\n",
    "The ```time``` command reported how long each operation took. For each data set, I used the series of commands\n",
    "\n",
    "```\n",
    "time gzip -k Filename\n",
    "time bzip2 -k Filename\n",
    "time pbzip2 -k Filename\n",
    "time ArithmeticCompress Filename Filename.art\n",
    "```\n",
    "\n",
    "```time``` reports ```real```, ```sys```, and ```user``` times. The table below only reports ```real``` time for each operation. \"Arith\" is short for ArithmeticCompress.\n",
    "\n",
    "### Results\n",
    "\n",
    "<img src=\"Part 2 Table.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Questions about Compression\n",
    "\n",
    "_Which algorithm achieves the best level of compression on each file type?_  \n",
    "```ArithmeticCompress``` compresses text files (100_Zeros, ..., 50_Zeros) and FASTA files best but also takes the longest time to complete.\n",
    "\n",
    "_Which algorithm is the fastest?_  \n",
    "```pbzip2``` compresses my data the fastest.\n",
    "\n",
    "_What is the difference between_ ```bzip2``` _and_ ```pbzip2```_? Do you expect one to be faster and why?_  \n",
    "```pbzip2``` is the parallel version of ```bzip2```. This means that ```pbzip2``` runs numerous ```bzip2``` algorithms simultaneously to compress files of interest. As such, it is quite clear that ```pbzip2``` is faster than ```bzip2```.\n",
    "\n",
    "_How does the level of compression change as the percentage of zeros increases? Why does this happen?_  \n",
    "Compression increases with increasing percentage of zeros. This is because as percentage of zeros increases, there is less \"unique\" data in the file. There is more statistical redundancy with increasing percentage of zeros, making compression easier.\n",
    "\n",
    "_What is the minimum number of bits required to store a single DNA base?_  \n",
    "2 bits, as log<sub>2</sub>(4) = 2\n",
    "\n",
    "_What is the minimum number of bits required to store a single amino acid letter?_  \n",
    "5 bits, as log<sub>2</sub>(20) â‰ˆ 4.322\n",
    "\n",
    "_In your tests, how many bits did ```gzip``` and ```bzip2``` actually require to store your random DNA and protein sequences?_  \n",
    "```gzip``` required 29.2 MB and 60.6 MB for the random DNA and protein sequences, respectively. ```bzip2``` required 27.3 MB and 55.3 MB for the same random DNA and protein sequences, respectively. \n",
    "\n",
    "_Are ```gzip``` and ```bzip2``` performing well on DNA and proteins?  \n",
    "Yes, considering the original filesize for the random DNA and protein sequences were both 100 MB. It seems that ```bzip2``` is slightly better for compressing the data, though it does take longer than ```gzip``` for more random data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compressing Real Data\n",
    "\n",
    "We now move on to interesting data. For this lab, I used **NCBI Nucleotide** to find 10 different HIV gp120 homologs. I concatenated their DNA sequences into a single multi-FASTA file ```HIV_gp120.fa```: GenBank AF236860.1, AF236859.1, AJ429917.1, AJ429906.1, AJ429920.1, AJ429919.1, AJ429915.1, AJ429913.1, AJ429909.1, and AJ429908.1.\n",
    "\n",
    "_A priori, do you expect to achieve better or worse compression here than random data? Why?_\n",
    "\n",
    "I expect better compression because biological data is _not random_. There are regions of nucleotide and protein sequences that have repeats (e.g. GC rich regions for higher DNA melt temperatures in thermophilic microbes) that are evolutionarily selected for. These repeats, along with other structural features, are statistically redundant and thus easier to compress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing the multi-FASTA using ```gzip```, ```bzip2```, and ```ArithmeticCompress```\n",
    "\n",
    "Like Part 2, I used **gzip**, **bzip2**, and **ArithmeticCompress** on **Terminal** using the commands:\n",
    "\n",
    "```\n",
    "time gzip -k HIV_gp120.fa\n",
    "time bzip2 -k HIV_gp120.fa\n",
    "time ArithmeticCompress HIV_gp120.fa HIV_gp120.fa.art\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "<img src=\"Part 4 Table.jpg\" width=\"50%\">\n",
    "\n",
    "_How does the compression ratio of this data compare to random data?_  \n",
    "\n",
    "The original filesize for ```HIV_gp120.fa``` is 12.4 kB. As such, the ```gzip```, ```bzip2```, and ```ArithmeticCompress``` compression ratios (compressed size divided by original size) are 25.2%, 25.2%, and 40.6%, respectively.\n",
    "\n",
    "Excluding the 100_Zeros random binary data, the compression ratio for this data is better than that of the random binary data (all >50%). Interestingly, the compression ratio for this data is better than the random DNA file when using ```gzip``` and ```bzip2```, though by a small margin (~4%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Estimations for Larger Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parameters_: 1000 TB of data is generated everyday. 80% of this data are genomes/plasmids with significant similarities, 10% are protein sequences, and 10% are random binary microscope images. I will earn a $500 bonus for every 1% reduction in data (10 TB), per day. Benchmark data for calculations referenced from Part 1.\n",
    "\n",
    "For genomes and plasmids, I would use ```pbzip2``` given its fast speed and relatively efficient compression of random DNA sequence data. The compression ratio for the random DNA FASTA file was 27.3 percent, with 100 MB compressed to 27.3 MB in 0.682s. We can thus compress 12.67 TB to 3.46 TB daily. **9.21 TB saved and $460.50 earned per day.**\n",
    "\n",
    "For protein sequences, I would use ```pbzip2``` for the same reasons as for genomes and plasmids. The compression ratio for the random protein FASTA file was 55.3 percent, and 100 MB was compressed to 55.3 MB in 0.802s. We can thus compress 10.77 TB to 5.96 TB daily. **4.81 TB saved and $240.50 earned per day.**\n",
    "\n",
    "Assuming the binary microscope images are completely random, compression is not possible. Complete randomness implies equal probability of ```0``` and ```1```, of which is modeled by the 50_Zeros benchmark data set.\n",
    "\n",
    "Using these algorithms, I will save 5117.3 TB (**1.402 percent of total data**) and earn $255865.00 per annum. Pretty good for some simple algorithms!\n",
    "\n",
    "These calculations are not accurate because they are premised on lackluster data, with error propagating quickly (the benchmark data are not representative biological data). Furthermore, different files have different compressibilities and these calculations assume only one operation is run per time interval. For an enterprise that generates 1000 TB, we would expect numerous computers to run numerous operations simultaneously. This makes it difficult to use my benchmark data to make any concrete conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
